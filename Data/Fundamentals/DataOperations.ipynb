{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Data Operations\n",
        "- Here is a collection of the different data operations that we perform on the different kinds of datasets using List, Pandas, Numpy & Scikit-learn."
      ],
      "metadata": {
        "id": "tacuXFzvDpME"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation and cleaning\n",
        "- Here we are cleaning the unwanted None values and then applying min - max scaling on the data to normalize the data values between 0 to 1.\n",
        "\n",
        "- Min - Max Scaling:\n",
        "\n",
        "  Normal_Value = (value - min(value)) / (max(value) - min(value))"
      ],
      "metadata": {
        "id": "1TSx7-99ISV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Implementation using a List\n",
        "# Defining a dataset using a vanilla list\n",
        "list_data = [\n",
        "    [5.0, 3.2, None],\n",
        "    [4.8, None, 2.1],\n",
        "    [None, 3.6, 2.4]\n",
        "]\n",
        "\n",
        "# This is a complex list comphension where it is excluding the None values, sums the values column wise and finds the average of the each row\n",
        "means = [\n",
        "    sum(filter(None, [row[i] for row in list_data])) / len([row[i] for row in list_data if row[i] is not None])\n",
        "    for i in range(len(list_data[0]))\n",
        "]\n",
        "\n",
        "cleaned_data = [\n",
        "    [row[i] if row[i] is not None else means[i] for i in range(len(row))]\n",
        "    for row in list_data\n",
        "]\n",
        "\n",
        "#Here traversing column wise on the transpose of the cleaned_data to pull out the minimun and maximun for each row\n",
        "min_values = [min(col) for col in zip(*cleaned_data)]\n",
        "max_values = [max(col) for col in zip(*cleaned_data)]\n",
        "\n",
        "# Normalizing the dataset for optimized operations\n",
        "normalized_data = [\n",
        "    [(val - min_values[i]) / (max_values[i] - min_values[i]) for i, val in enumerate(row)]\n",
        "    for row in cleaned_data\n",
        "]\n",
        "\n",
        "print(\"Normalized Data:\", normalized_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbIm9PguI9-p",
        "outputId": "10868ebe-921d-408c-bc94-2605a207d2aa"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Data: [[1.0, 0.0, 0.5], [0.0, 0.5000000000000006, 0.0], [0.5000000000000022, 1.0, 1.0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Implementation using a Numpy array\n",
        "# Defining a dataset using a Numpy array\n",
        "import numpy as np\n",
        "\n",
        "np_data = np.array([\n",
        "    [5.0, 3.2, np.nan],\n",
        "    [4.8, np.nan, 2.1],\n",
        "    [np.nan, 3.6, 2.4]\n",
        "])\n",
        "\n",
        "# Replace NaNs with column means\n",
        "col_means = np.nanmean(np_data, axis=0)\n",
        "data = np.where(np.isnan(np_data), col_means, np_data)\n",
        "\n",
        "# Normalize using Min-Max scaling\n",
        "data_min = data.min(axis=0)\n",
        "data_max = data.max(axis=0)\n",
        "normalized_data = (data - data_min) / (data_max - data_min)\n",
        "\n",
        "print(\"Normalized Data:\", normalized_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgp7s0-WSSho",
        "outputId": "97590b1c-3541-430f-ca7a-8a4b85c0305a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Data: [[1.  0.  0.5]\n",
            " [0.  0.5 0. ]\n",
            " [0.5 1.  1. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Implementation using a Pandas dataframe\n",
        "# Defining a dataset using a Pandas dataframe\n",
        "import pandas as pd\n",
        "\n",
        "# Convert data to Pandas DataFrame\n",
        "pd_data = pd.DataFrame({\n",
        "    \"Feature1\": [5.0, 4.8, None],\n",
        "    \"Feature2\": [3.2, None, 3.6],\n",
        "    \"Feature3\": [None, 2.1, 2.4]\n",
        "})\n",
        "\n",
        "# Fill missing values with column means\n",
        "pd_data.fillna(pd_data.mean(), inplace=True)\n",
        "\n",
        "# Normalize using Min-Max scaling\n",
        "normalized_data = (data - data.min()) / (data.max() - data.min())\n",
        "\n",
        "print(\"Normalized Data:\\n\", normalized_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8w3rsHcUSUM",
        "outputId": "2b707f52-b921-4e90-a66c-183ae5a9db40"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized Data:\n",
            " [[1.         0.37931034 0.05172414]\n",
            " [0.93103448 0.44827586 0.        ]\n",
            " [0.96551724 0.51724138 0.10344828]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering\n",
        "- Encode categorical data into numerical data. Using Encoders\n",
        "- Scale numerical features into managable values. Using Scaler"
      ],
      "metadata": {
        "id": "o__ejDjbIZRa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Implementing using a numpy array\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "# Categorical data\n",
        "categories = np.array([\"low\", \"medium\", \"high\", \"medium\", \"low\"])\n",
        "\n",
        "# Encode categories to integers\n",
        "le = LabelEncoder()\n",
        "encoded_categories = le.fit_transform(categories)\n",
        "\n",
        "# Numerical data\n",
        "numerical_data = np.array([[5.5, 2.3], [6.1, 3.4], [4.8, 1.2]])\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "scaled_numerical = scaler.fit_transform(numerical_data)\n",
        "\n",
        "print(\"Encoded Categories:\", encoded_categories)\n",
        "print(\"Scaled Numerical Data:\\n\", scaled_numerical)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZthb-jlXul9",
        "outputId": "0e85fc53-9f64-44a6-bd3b-b410079a51a6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded Categories: [1 2 0 2 1]\n",
            "Scaled Numerical Data:\n",
            " [[ 0.06274558  0.        ]\n",
            " [ 1.19216603  1.22474487]\n",
            " [-1.25491161 -1.22474487]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical and numerical data\n",
        "pdf_data = pd.DataFrame({\n",
        "    \"Category\": [\"low\", \"medium\", \"high\", \"medium\", \"low\"],\n",
        "    \"Feature1\": [5.5, 6.1, 4.8, 7.0, 5.2],\n",
        "    \"Feature2\": [2.3, 3.4, 1.2, 2.8, 2.5]\n",
        "})\n",
        "\n",
        "# Encode categories\n",
        "pdf_data[\"Category\"] = pdf_data[\"Category\"].astype(\"category\").cat.codes\n",
        "\n",
        "# Scale numerical features\n",
        "pdf_data[[\"Feature1\", \"Feature2\"]] = (pdf_data[[\"Feature1\", \"Feature2\"]] - pdf_data[[\"Feature1\", \"Feature2\"]].mean()) / pdf_data[[\"Feature1\", \"Feature2\"]].std()\n",
        "\n",
        "print(\"Transformed Data:\\n\", pdf_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dTe6GlfaAEC",
        "outputId": "b5f8047c-f9f6-4c89-d16f-f088e1b2fd31"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformed Data:\n",
            "    Category  Feature1  Feature2\n",
            "0         1 -0.256265 -0.173249\n",
            "1         2  0.442639  1.187995\n",
            "2         0 -1.071653 -1.534494\n",
            "3         2  1.490995  0.445498\n",
            "4         1 -0.605717  0.074250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sparse Data handling"
      ],
      "metadata": {
        "id": "spAytMSNIeLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Create a sparse matrix\n",
        "dense_matrix = np.array([\n",
        "    [0, 0, 3],\n",
        "    [0, 0, 0],\n",
        "    [7, 0, 0]\n",
        "])\n",
        "sparse_matrix = csr_matrix(dense_matrix)\n",
        "\n",
        "# Operations on sparse matrix\n",
        "print(\"Sparse Matrix:\\n\", sparse_matrix)\n",
        "print(\"Dense Representation:\\n\", sparse_matrix.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLFjetjgalUW",
        "outputId": "6618e9fb-4fa0-4aaa-b5e3-bbafd0171c81"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sparse Matrix:\n",
            "   (0, 2)\t3\n",
            "  (2, 0)\t7\n",
            "Dense Representation:\n",
            " [[0 0 3]\n",
            " [0 0 0]\n",
            " [7 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model input preparation"
      ],
      "metadata": {
        "id": "VBQeG93OIlfT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(\"Train Features Shape:\", X_train.shape)\n",
        "print(\"Test Labels Shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sQCIkZIEa4au",
        "outputId": "9463e910-bbf9-40ef-ec60-6c3b1e9009b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Features Shape: (120, 4)\n",
            "Test Labels Shape: (30,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handling Multi-Dimensional Data with Tensors"
      ],
      "metadata": {
        "id": "Nj8wnBN-I0UP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Simulating random image data: Batch of 3 images, RGB channels, 64x64 resolution\n",
        "images = torch.rand(3, 3, 64, 64)\n",
        "\n",
        "# Checking dimensions\n",
        "print(\"Shape of Images Tensor:\", images.shape)\n",
        "\n",
        "# Normalize pixel values (0-1 range)\n",
        "# Min - Max Scaling\n",
        "normalized_images = (images - images.min()) / (images.max() - images.min())\n",
        "\n",
        "# Reshape for model input (if needed)\n",
        "reshaped_images = normalized_images.view(-1, 3, 64, 64)\n"
      ],
      "metadata": {
        "id": "9pLJO3yhbF6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Pipeline with Pandas and NumPy\n",
        "-> Load Data -> Handle missing values -> Enocode catagorical features -> Scale numerical features -> Split Dataset"
      ],
      "metadata": {
        "id": "Q10vk829cw62"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "pipe_data = pd.DataFrame({\n",
        "    \"Category\": [\"low\", \"medium\", \"high\", \"medium\", \"low\", \"high\"],\n",
        "    \"Feature1\": [5.5, 6.1, np.nan, 7.0, 5.2, 6.8],\n",
        "    \"Feature2\": [2.3, 3.4, 1.2, np.nan, 2.5, 3.1],\n",
        "    \"Target\": [0, 1, 0, 1, 0, 1]\n",
        "})\n",
        "\n",
        "# 1. Handle missing values\n",
        "pipe_data.fillna(pipe_data.mean(), inplace=True)\n",
        "\n",
        "# 2. Encode categorical features\n",
        "le = LabelEncoder()\n",
        "pipe_data[\"Category\"] = le.fit_transform(pipe_data[\"Category\"])\n",
        "\n",
        "# 3. Scale numerical features\n",
        "scaler = StandardScaler()\n",
        "data[[\"Feature1\", \"Feature2\"]] = scaler.fit_transform(data[[\"Feature1\", \"Feature2\"]])\n",
        "\n",
        "# 4. Prepare features and labels\n",
        "X = data[[\"Category\", \"Feature1\", \"Feature2\"]].values\n",
        "y = data[\"Target\"].values\n",
        "\n",
        "# 5. Split into training and testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "print(\"Training Features:\\n\", X_train)\n",
        "print(\"Training Labels:\\n\", y_train)"
      ],
      "metadata": {
        "id": "Z299C2KQdIAW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}